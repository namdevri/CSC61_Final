{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DSP 461 Final Project v3.ipynb","provenance":[{"file_id":"1fF72BlAm7vwFnC9cCnAjMHb_0ZhK23wy","timestamp":1575235128854},{"file_id":"1yyL8Uo9SzZhV6HtAEuEGPYNoUlOYa3aK","timestamp":1574796892120}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZXLMLqmhqGqt","colab_type":"code","outputId":"9eb99ae7-3fa9-48ff-b906-5a26c69a6b7d","executionInfo":{"status":"ok","timestamp":1575232281073,"user_tz":300,"elapsed":2450,"user":{"displayName":"Joseph Erickson","photoUrl":"","userId":"18197697941711049711"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","#mount your drive.  Complete Oauth to authenticate\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hqJt5FExqfYg","colab_type":"code","colab":{}},"source":["#unzip image folder\n","!unzip -uq \"/content/gdrive/My Drive/jpegs.zip\" -d \"/content/gdrive/My Drive/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qA-s2tHpDc2e","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"xC81Yd_KrO5M","colab_type":"code","outputId":"140a7989-c69e-4023-a1a0-40a22710afc4","executionInfo":{"status":"error","timestamp":1574790004810,"user_tz":300,"elapsed":11764,"user":{"displayName":"Joseph Erickson","photoUrl":"","userId":"18197697941711049711"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import torch\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Training set values (same for all data)\n","data_means = [0.6786, 0.6413, 0.6605]\n","data_stds = [0.2012, 0.2080, 0.1997]\n","\n","transformations = transforms.Compose([\n","#    transforms.Resize(255),\n","#    transforms.CenterCrop(224),\n","    transforms.ToTensor(),  # Transforms channels from 0- 255 -> 0-1.\n","    transforms.Normalize(mean=data_means, std=data_stds)])\n","\n","train_set = datasets.ImageFolder(\"/content/gdrive/My Drive/TRAIN\", transform=transformations)\n","print(\"Train set size: \", len(train_set))\n","#train_set_E = datasets.ImageFolder(\"/content/gdrive/My Drive/TRAIN/EOSINOPHIL\", transform=transformations)\n","#train_set_L = datasets.ImageFolder(\"/content/gdrive/My Drive/TRAIN/LYMPHOCYTE\", transform=transformations)\n","#train_set_M = datasets.ImageFolder(\"/content/gdrive/My Drive/TRAIN/MONOCYTE\", transform=transformations)\n","#train_set_N = datasets.ImageFolder(\"/content/gdrive/My Drive/TRAIN/NEUTROPHIL\", transform=transformations)\n","\n","num_folds = 4\n","folds = []\n","for i in range(num_folds):\n","    fold_length = int(len(train_set) / (1.*num_folds - i))\n","    train_set, new_fold = torch.utils.data.random_split(train_set, [len(train_set) - fold_length, fold_length])\n","    folds.append(new_fold)\n","    print(\"Fold size: \", len(new_fold))\n","\n","# Will load data\n","#train_loader = torch.utils.data.DataLoader(train_set, batch_size=25, shuffle=True)\n","\n","test_set = datasets.ImageFolder(\"/content/gdrive/My Drive/TEST\", transform=transformations)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size =25, shuffle=True)\n","\n","# Options: MOST GENERALIZED - 121, 169, 201, 161 - MOST ACCURATE\n","# https://pytorch.org/hub/pytorch_vision_densenet/\n","model = models.densenet161(pretrained=True)\n","\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","classifier_input = model.classifier.in_features\n","num_labels = 4\n","classifier = nn.Sequential(nn.Linear(classifier_input, 1024),\n","                           nn.ReLU(),\n","                           nn.Linear(1024, 512),\n","                           nn.ReLU(),\n","                           nn.Linear(512, num_labels),\n","                           nn.LogSoftmax(dim=1))\n","\n","\n","model.classifier = classifier\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model.to(device)\n","\n","criterion = nn.NLLLoss()\n","# Set the optimizer function using torch.optim as optim library\n","optimizer = optim.Adam(model.classifier.parameters())\n","\n","epochs = 10\n","for epoch in range(epochs):\n","    ep_train_loss = 0\n","    ep_val_loss = 0\n","    ep_accuracy = 0\n","\n","    for i in range(num_folds):\n","\n","        train_loss = 0\n","        val_loss = 0\n","        accuracy = 0\n","\n","        # Constructing training and validation sets\n","\n","        train_set = -1\n","        for j in range(num_folds):\n","            if j != i:\n","                if train_set == -1:\n","                    train_set = folds[j]\n","                else:\n","                    train_set = torch.utils.data.ConcatDataset([train_set, folds[j]])\n","        val_set = folds[i]\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=25, shuffle=True)\n","        val_loader = torch.utils.data.DataLoader(val_set, batch_size=25, shuffle=True)\n","\n","        # Training the model\n","        model.train()\n","        counter = 0\n","        for inputs, labels in train_loader:\n","            # Move to device\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            # Clear optimizers\n","            optimizer.zero_grad()\n","            # Forward pass\n","            output = model.forward(inputs)\n","            # Loss\n","            loss = criterion(output, labels)\n","            # Calculate gradients (backpropogation)\n","            loss.backward()\n","            # Adjust parameters based on gradients\n","            optimizer.step()\n","            # Add the loss to the training set's rnning loss\n","            train_loss += loss.item()*inputs.size(0)\n","\n","            # Print the progress of our training\n","            counter += 1\n","            print(counter, \"/\", len(train_loader))\n","\n","        # Evaluating the model\n","        model.eval()\n","        counter = 0\n","        # Tell torch not to calculate gradients\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                # Move to device\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                # Forward pass\n","                output = model.forward(inputs)\n","                # Calculate Loss\n","                valloss = criterion(output, labels)\n","                # Add loss to the validation set's running loss\n","                val_loss += valloss.item()*inputs.size(0)\n","\n","                # Since our model outputs a LogSoftmax, find the real\n","                # percentages by reversing the log function\n","                output = torch.exp(output)\n","                # Get the top class of the output\n","                top_p, top_class = output.topk(1, dim=1)\n","                # See how many of the classes were correct?\n","                equals = top_class == labels.view(*top_class.shape)\n","                # Calculate the mean (get the accuracy for this batch)\n","                # and add it to the running accuracy for this epoch\n","                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","                # Print the progress of our evaluation\n","                counter += 1\n","                print(counter, \"/\", len(val_loader))\n","\n","        # Get the average loss for the entire fold\n","        train_loss = train_loss/len(train_loader.dataset)\n","        val_loss = val_loss/len(val_loader.dataset)\n","        accuracy = accuracy/len(val_loader)\n","        # Add to epoch's running total for avg losses\n","        ep_train_loss += train_loss\n","        ep_val_loss += val_loss\n","        ep_accuracy += accuracy\n","        # Print out the information\n","        print('Fold Accuracy: ', accuracy, '\\n')\n","        print('Epoch: {} \\tFold: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, i, train_loss, val_loss))\n","\n","    # Get the average loss for the entire fold\n","    ep_train_loss = ep_train_loss/num_folds\n","    ep_val_loss = ep_val_loss/num_folds\n","    ep_accuracy = ep_accuracy/num_folds\n","    # Print out the information\n","    print('\\n\\t*** Epoch Accuracy: ', ep_accuracy)\n","    print('\\t*** Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\n'.format(epoch, ep_train_loss, val_loss))\n","\n","torch.save(model, \"./blood_model.py\")\n","\n","\n","model.eval()\n","# Process our image\n","def process_image(image_path):\n","    # Load Image\n","    img = Image.open(image_path)\n","\n","    # Get the dimensions of the image\n","    width, height = img.size\n","\n","    # Resize by keeping the aspect ratio, but changing the dimension\n","    # so the shortest size is 255px\n","    # img = img.resize((255, int(255*(height/width))) if width < height else (int(255*(width/height)), 255))\n","\n","    # Get the dimensions of the new image size\n","    width, height = img.size\n","\n","    # Set the coordinates to do a center crop of 224 x 224\n","    #left = (width - 224)/2\n","    #top = (height - 224)/2\n","    #right = (width + 224)/2\n","    #bottom = (height + 224)/2\n","    #img = img.crop((left, top, right, bottom))\n","\n","    # Turn image into numpy array\n","    img = np.array(img)\n","\n","    # Make the color channel dimension first instead of last\n","    img = img.transpose((2, 0, 1))\n","\n","    # Make all values between 0 and 1\n","    img = img/255\n","\n","    # Normalize based on the preset mean and standard deviation\n","    img[0] = (img[0] - data_means[0])/data_stds[0]\n","    img[1] = (img[1] - data_means[1])/data_stds[1]\n","    img[2] = (img[2] - data_means[2])/data_stds[2]\n","\n","    # Add a fourth dimension to the beginning to indicate batch size\n","    img = img[np.newaxis,:]\n","\n","    # Turn into a torch tensor\n","    image = torch.from_numpy(img)\n","    image = image.float()\n","    return image\n","\n","# Using our model to predict the label\n","def predict(image, model):\n","    # Pass the image through our model\n","    output = model.forward(image)\n","\n","    # Reverse the log function in our output\n","    output = torch.exp(output)\n","\n","    # Get the top predicted class, and the output percentage for\n","    # that class\n","    probs, classes = output.topk(1, dim=1)\n","    return probs.item(), classes.item()\n","\n","# Show Image\n","def show_image(image):\n","    # Convert image to numpy\n","    image = image.numpy()\n","\n","    # Un-normalize the image with avg std and mean\n","    image[0] = image[0] * 0.2030 + 0.6601\n","\n","    # Print the image\n","    fig = plt.figure(figsize=(25, 4))\n","    plt.imshow(np.transpose(image[0], (1, 2, 0)))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train set size:  9957\n","Fold size:  2489\n","Fold size:  2489\n","Fold size:  2489\n","Fold size:  2490\n","1 / 299\n","2 / 299\n","3 / 299\n","4 / 299\n","5 / 299\n","6 / 299\n","7 / 299\n","8 / 299\n","9 / 299\n","10 / 299\n","11 / 299\n","12 / 299\n","13 / 299\n","14 / 299\n","15 / 299\n","16 / 299\n","17 / 299\n","18 / 299\n","19 / 299\n","20 / 299\n","21 / 299\n","22 / 299\n","23 / 299\n","24 / 299\n","25 / 299\n","26 / 299\n","27 / 299\n","28 / 299\n","29 / 299\n","30 / 299\n","31 / 299\n","32 / 299\n","33 / 299\n","34 / 299\n","35 / 299\n","36 / 299\n","37 / 299\n","38 / 299\n","39 / 299\n","40 / 299\n","41 / 299\n","42 / 299\n","43 / 299\n","44 / 299\n","45 / 299\n","46 / 299\n","47 / 299\n","48 / 299\n","49 / 299\n","50 / 299\n","51 / 299\n","52 / 299\n","53 / 299\n","54 / 299\n","55 / 299\n","56 / 299\n","57 / 299\n","58 / 299\n","59 / 299\n","60 / 299\n","61 / 299\n","62 / 299\n","63 / 299\n","64 / 299\n","65 / 299\n","66 / 299\n","67 / 299\n","68 / 299\n","69 / 299\n","70 / 299\n","71 / 299\n","72 / 299\n","73 / 299\n","74 / 299\n","75 / 299\n","76 / 299\n","77 / 299\n","78 / 299\n","79 / 299\n","80 / 299\n","81 / 299\n","82 / 299\n","83 / 299\n","84 / 299\n","85 / 299\n","86 / 299\n","87 / 299\n","88 / 299\n","89 / 299\n","90 / 299\n","91 / 299\n","92 / 299\n","93 / 299\n","94 / 299\n","95 / 299\n","96 / 299\n","97 / 299\n","98 / 299\n","99 / 299\n","100 / 299\n","101 / 299\n","102 / 299\n","103 / 299\n","104 / 299\n","105 / 299\n","106 / 299\n","107 / 299\n","108 / 299\n","109 / 299\n","110 / 299\n","111 / 299\n","112 / 299\n","113 / 299\n","114 / 299\n","115 / 299\n","116 / 299\n","117 / 299\n","118 / 299\n","119 / 299\n","120 / 299\n","121 / 299\n","122 / 299\n","123 / 299\n","124 / 299\n","125 / 299\n","126 / 299\n","127 / 299\n","128 / 299\n","129 / 299\n","130 / 299\n","131 / 299\n","132 / 299\n","133 / 299\n","134 / 299\n","135 / 299\n","136 / 299\n","137 / 299\n","138 / 299\n","139 / 299\n","140 / 299\n","141 / 299\n","142 / 299\n","143 / 299\n","144 / 299\n","145 / 299\n","146 / 299\n","147 / 299\n","148 / 299\n","149 / 299\n","150 / 299\n","151 / 299\n","152 / 299\n","153 / 299\n","154 / 299\n","155 / 299\n","156 / 299\n","157 / 299\n","158 / 299\n","159 / 299\n","160 / 299\n","161 / 299\n","162 / 299\n","163 / 299\n","164 / 299\n","165 / 299\n","166 / 299\n","167 / 299\n","168 / 299\n","169 / 299\n","170 / 299\n","171 / 299\n","172 / 299\n","173 / 299\n","174 / 299\n","175 / 299\n","176 / 299\n","177 / 299\n","178 / 299\n","179 / 299\n","180 / 299\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SkXd_yqkv6hr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}