{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DSP 461 Final Project.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZXLMLqmhqGqt","colab_type":"code","outputId":"ec39125e-ec3d-4109-bcad-993ca1040ec2","executionInfo":{"status":"ok","timestamp":1574787721326,"user_tz":300,"elapsed":37218,"user":{"displayName":"Joseph Erickson","photoUrl":"","userId":"18197697941711049711"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["from google.colab import drive\n","#mount your drive.  Complete Oauth to authenticate\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hqJt5FExqfYg","colab_type":"code","colab":{}},"source":["#unzip image folder\n","!unzip -uq \"/content/gdrive/My Drive/jpegs.zip\" -d \"/content/gdrive/My Drive/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qA-s2tHpDc2e","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"xC81Yd_KrO5M","colab_type":"code","outputId":"3e542058-81f9-4d5b-c970-d506a570ea51","executionInfo":{"status":"error","timestamp":1574790004810,"user_tz":300,"elapsed":11764,"user":{"displayName":"Joseph Erickson","photoUrl":"","userId":"18197697941711049711"}},"colab":{"base_uri":"https://localhost:8080/","height":432}},"source":["import torch\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Training set values (same for all data)\n","data_means = [0.6786, 0.6413, 0.6605]\n","data_stds = [0.2012, 0.2080, 0.1997]\n","\n","transformations = transforms.Compose([\n","#    transforms.Resize(255),\n","#    transforms.CenterCrop(224),\n","    transforms.ToTensor(),  # Transforms channels from 0- 255 -> 0-1.\n","    transforms.Normalize(mean=data_means, std=data_stds)])\n","\n","train_set = datasets.ImageFolder(\"/content/gdrive/My Drive/TRAIN\", transform=transformations)\n","test_set = datasets.ImageFolder(\"/content/gdrive/My Drive/TEST\", transform=transformations)\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=25, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size =25, shuffle=True)\n","\n","# Options: MOST GENERALIZED - 121, 169, 201, 161 - MOST ACCURATE\n","# https://pytorch.org/hub/pytorch_vision_densenet/\n","model = models.densenet161(pretrained=True)\n","\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","classifier_input = model.classifier.in_features\n","num_labels = 4\n","classifier = nn.Sequential(nn.Linear(classifier_input, 1024),\n","                           nn.ReLU(),\n","                           nn.Linear(1024, 512),\n","                           nn.ReLU(),\n","                           nn.Linear(512, num_labels),\n","                           nn.LogSoftmax(dim=1))\n","\n","\n","model.classifier = classifier\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model.to(device)\n","\n","criterion = nn.NLLLoss()\n","# Set the optimizer function using torch.optim as optim library\n","optimizer = optim.Adam(model.classifier.parameters())\n","\n","epochs = 10\n","for epoch in range(epochs):\n","    train_loss = 0\n","    val_loss = 0\n","    accuracy = 0\n","\n","    # Training the model\n","    model.train()\n","    counter = 0\n","    for inputs, labels in train_loader:\n","        # Move to device\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        # Clear optimizers\n","        optimizer.zero_grad()\n","        # Forward pass\n","        output = model.forward(inputs)\n","        # Loss\n","        loss = criterion(output, labels)\n","        # Calculate gradients (backpropogation)\n","        loss.backward()\n","        # Adjust parameters based on gradients\n","        optimizer.step()\n","        # Add the loss to the training set's rnning loss\n","        train_loss += loss.item()*inputs.size(0)\n","\n","        # Print the progress of our training\n","        counter += 1\n","        print(counter, \"/\", len(train_loader))\n","\n","    # Evaluating the model\n","    model.eval()\n","    counter = 0\n","    # Tell torch not to calculate gradients\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            # Move to device\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            # Forward pass\n","            output = model.forward(inputs)\n","            # Calculate Loss\n","            valloss = criterion(output, labels)\n","            # Add loss to the validation set's running loss\n","            val_loss += valloss.item()*inputs.size(0)\n","\n","            # Since our model outputs a LogSoftmax, find the real\n","            # percentages by reversing the log function\n","            output = torch.exp(output)\n","            # Get the top class of the output\n","            top_p, top_class = output.topk(1, dim=1)\n","            # See how many of the classes were correct?\n","            equals = top_class == labels.view(*top_class.shape)\n","            # Calculate the mean (get the accuracy for this batch)\n","            # and add it to the running accuracy for this epoch\n","            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","            # Print the progress of our evaluation\n","            counter += 1\n","            print(counter, \"/\", len(test_loader))\n","\n","    # Get the average loss for the entire epoch\n","    train_loss = train_loss/len(train_loader.dataset)\n","    valid_loss = val_loss/len(test_loader.dataset)\n","    # Print out the information\n","    print('Accuracy: ', accuracy/len(test_loader))\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n","\n","\n","torch.save(model, \"./blood_model.py\")\n","\n","\n","model.eval()\n","# Process our image\n","def process_image(image_path):\n","    # Load Image\n","    img = Image.open(image_path)\n","\n","    # Get the dimensions of the image\n","    width, height = img.size\n","\n","    # Resize by keeping the aspect ratio, but changing the dimension\n","    # so the shortest size is 255px\n","    # img = img.resize((255, int(255*(height/width))) if width < height else (int(255*(width/height)), 255))\n","\n","    # Get the dimensions of the new image size\n","    width, height = img.size\n","\n","    # Set the coordinates to do a center crop of 224 x 224\n","    #left = (width - 224)/2\n","    #top = (height - 224)/2\n","    #right = (width + 224)/2\n","    #bottom = (height + 224)/2\n","    #img = img.crop((left, top, right, bottom))\n","\n","    # Turn image into numpy array\n","    img = np.array(img)\n","\n","    # Make the color channel dimension first instead of last\n","    img = img.transpose((2, 0, 1))\n","\n","    # Make all values between 0 and 1\n","    img = img/255\n","\n","    # Normalize based on the preset mean and standard deviation\n","    img[0] = (img[0] - data_means[0])/data_stds[0]\n","    img[1] = (img[1] - data_means[1])/data_stds[1]\n","    img[2] = (img[2] - data_means[2])/data_stds[2]\n","\n","    # Add a fourth dimension to the beginning to indicate batch size\n","    img = img[np.newaxis,:]\n","\n","    # Turn into a torch tensor\n","    image = torch.from_numpy(img)\n","    image = image.float()\n","    return image\n","\n","# Using our model to predict the label\n","def predict(image, model):\n","    # Pass the image through our model\n","    output = model.forward(image)\n","\n","    # Reverse the log function in our output\n","    output = torch.exp(output)\n","\n","    # Get the top predicted class, and the output percentage for\n","    # that class\n","    probs, classes = output.topk(1, dim=1)\n","    return probs.item(), classes.item()\n","\n","# Show Image\n","def show_image(image):\n","    # Convert image to numpy\n","    image = image.numpy()\n","\n","    # Un-normalize the image with avg std and mean\n","    image[0] = image[0] * 0.2030 + 0.6601\n","\n","    # Print the image\n","    fig = plt.figure(figsize=(25, 4))\n","    plt.imshow(np.transpose(image[0], (1, 2, 0)))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1 / 399\n","2 / 399\n","3 / 399\n","4 / 399\n","5 / 399\n","6 / 399\n","7 / 399\n","8 / 399\n","9 / 399\n","10 / 399\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-d628db49e6f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Add the loss to the training set's rnning loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Print the progress of our training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"SkXd_yqkv6hr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}