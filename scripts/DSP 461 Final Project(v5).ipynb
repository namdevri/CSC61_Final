{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DSP 461 Final Project(v5).ipynb","provenance":[{"file_id":"1fF72BlAm7vwFnC9cCnAjMHb_0ZhK23wy","timestamp":1575498877653},{"file_id":"1yyL8Uo9SzZhV6HtAEuEGPYNoUlOYa3aK","timestamp":1574796892120}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZXLMLqmhqGqt","colab_type":"code","outputId":"3a30e37b-1063-4ffd-f6ec-8c1bb5f3de79","executionInfo":{"status":"ok","timestamp":1575499850650,"user_tz":300,"elapsed":22033,"user":{"displayName":"Joseph Erickson","photoUrl":"","userId":"18197697941711049711"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["from google.colab import drive\n","#mount your drive.  Complete Oauth to authenticate\n","drive.mount('/content/gdrive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hqJt5FExqfYg","colab_type":"code","colab":{}},"source":["#unzip image folder\n","#!unzip -uq \"/content/gdrive/My Drive/jpegs.zip\" -d \"/content/gdrive/My Drive/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qA-s2tHpDc2e","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"xC81Yd_KrO5M","colab_type":"code","outputId":"b4b60ce5-eaca-4946-c9db-c4adb81f744f","executionInfo":{"status":"error","timestamp":1575496737858,"user_tz":300,"elapsed":1137,"user":{"displayName":"Joseph Erickson","photoUrl":"","userId":"18197697941711049711"}},"colab":{"base_uri":"https://localhost:8080/","height":239}},"source":["import torch\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Training set values (same for all data)\n","data_means = [0.6786, 0.6413, 0.6605]\n","data_stds = [0.2012, 0.2080, 0.1997]\n","\n","transformations = transforms.Compose([\n","#    transforms.Resize(255),\n","#    transforms.CenterCrop(224),\n","    transforms.ToTensor(),  # Transforms channels from 0- 255 -> 0-1.\n","    transforms.Normalize(mean=data_means, std=data_stds)])\n","\n","epochs = 10\n","val_percent = 0.2\n","train_batch_size = 25\n","test_batch_size = 25\n","\n","full_train_set = datasets.ImageFolder(\"/content/gdrive/My Drive/TRAIN\", transform=transformations)\n","#full_train_set, temp = torch.utils.data.random_split(full_train_set, [int(len(full_train_set) / 20), len(full_train_set) - int(len(full_train_set) / 20)])\n","full_train_loader = torch.utils.data.DataLoader(full_train_set, batch_size=train_batch_size, shuffle=True)\n","print(\"Train set size: \", len(full_train_set))\n","\n","val_size = int(len(full_train_set)*val_percent)\n","train_set, val_set = torch.utils.data.random_split(full_train_set, [len(full_train_set) - val_size, val_size])\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=train_batch_size, shuffle=True)\n","\n","test_set = datasets.ImageFolder(\"/content/gdrive/My Drive/TEST\", transform=transformations)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=test_batch_size, shuffle=True)\n","\n","# Options: MOST GENERALIZED - 121, 169, 201, 161 - MOST ACCURATE\n","# https://pytorch.org/hub/pytorch_vision_densenet/\n","model = models.densenet161(pretrained=True)\n","\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","classifier_input = model.classifier.in_features\n","num_labels = 4\n","classifier = nn.Sequential(nn.Linear(classifier_input, 1024),\n","                           nn.ReLU(),\n","                           nn.Linear(1024, 512),\n","                           nn.ReLU(),\n","                           nn.Linear(512, num_labels),\n","                           nn.LogSoftmax(dim=1))\n","\n","\n","model.classifier = classifier\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model.to(device)\n","\n","# Can choose various loss functions to use.\n","criterion = nn.NLLLoss()\n","# Set the optimizer function using torch.optim as optim library\n","optimizer = optim.Adam(model.classifier.parameters())\n","\n","for epoch in range(epochs):\n","    train_loss = 0\n","    val_loss = 0\n","    accuracy = 0\n","\n","    # Training the model\n","    model.train()\n","    counter = 0\n","    for inputs, labels in train_loader:\n","        # Move to device\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        # Clear optimizers\n","        optimizer.zero_grad()\n","        # Forward pass\n","        output = model.forward(inputs)\n","        # Loss\n","        loss = criterion(output, labels)\n","        # Calculate gradients (backpropogation)\n","        loss.backward()\n","        # Adjust parameters based on gradients\n","        optimizer.step()\n","        # Add the loss to the training set's rnning loss\n","        train_loss += loss.item()*inputs.size(0)\n","\n","        # Print the progress of our training\n","        counter += 1\n","        if epoch == 0:\n","            print(counter, \"/\", len(train_loader))\n","\n","    # Evaluating the model\n","    model.eval()\n","    counter = 0\n","    # Tell torch not to calculate gradients\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            # Move to device\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            # Forward pass\n","            output = model.forward(inputs)\n","            # Calculate Loss\n","            valloss = criterion(output, labels)\n","            # Add loss to the validation set's running loss\n","            val_loss += valloss.item()*inputs.size(0)\n","\n","            # Since our model outputs a LogSoftmax, find the real\n","            # percentages by reversing the log function\n","            output = torch.exp(output)\n","            # Get the top class of the output\n","            top_p, top_class = output.topk(1, dim=1)\n","            # See how many of the classes were correct?\n","            equals = top_class == labels.view(*top_class.shape)\n","            # Calculate the mean (get the accuracy for this batch)\n","            # and add it to the running accuracy for this epoch\n","            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","            # Print the progress of our evaluation\n","            counter += 1\n","            if epoch == 0:\n","                print(counter, \"/\", len(val_loader))\n","\n","    # Get the average loss for the entire fold\n","    train_loss = train_loss/len(train_loader.dataset)\n","    val_loss = val_loss/len(val_loader.dataset)\n","    accuracy = accuracy/len(val_loader)\n","    # Print out the information\n","    print('Epoch: {} \\tFold {} Accuracy: {:.6f} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\n'.format(epoch+1, accuracy, train_loss, val_loss))\n","\n","\n","\n","full_train_loss = 0\n","test_loss = 0\n","test_accuracy = 0\n","\n","# Training the model one final time on the full dataset\n","model.train()\n","counter = 0\n","for inputs, labels in full_train_loader:\n","    # Move to device\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    # Clear optimizers\n","    optimizer.zero_grad()\n","    # Forward pass\n","    output = model.forward(inputs)\n","    # Loss\n","    loss = criterion(output, labels)\n","    # Calculate gradients (backpropogation)\n","    loss.backward()\n","    # Adjust parameters based on gradients\n","    optimizer.step()\n","    # Add the loss to the training set's rnning loss\n","    full_train_loss += loss.item()*inputs.size(0)\n","\n","    # Print the progress of our training\n","    counter += 1\n","    print(counter, \"/\", len(full_train_loader))\n","\n","# Saving the model\n","torch.save(model, \"./blood_model.py\")\n","\n","\n","model.eval()\n","counter = 0\n","# Tell torch not to calculate gradients\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        # Move to device\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        # Forward pass\n","        output = model.forward(inputs)\n","        # Calculate Loss\n","        testloss = criterion(output, labels)\n","        # Add loss to the validation set's running loss\n","        test_loss += testloss.item()*inputs.size(0)\n","\n","        # Since our model outputs a LogSoftmax, find the real\n","        # percentages by reversing the log function\n","        output = torch.exp(output)\n","        # Get the top class of the output\n","        top_p, top_class = output.topk(1, dim=1)\n","        # See how many of the classes were correct?\n","        equals = top_class == labels.view(*top_class.shape)\n","        # Calculate the mean (get the accuracy for this batch)\n","        # and add it to the running accuracy for this epoch\n","        test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","        # Print the progress of our evaluation\n","        counter += 1\n","        print(counter, \"/\", len(test_loader))\n","\n","# Get the average loss for the entire fold\n","full_train_loss = full_train_loss/len(full_train_loader.dataset)\n","test_loss = test_loss/len(test_loader.dataset)\n","test_accuracy = test_accuracy/len(test_loader)\n","# Print out the information\n","print('Test Set Accuracy: ', test_accuracy)\n","print('Training Loss: {:.6f} \\tTesting Loss: {:.6f} \\n'.format(full_train_loss, test_loss))\n","\n","\n","# Process our image\n","def process_image(image_path):\n","    # Load Image\n","    img = Image.open(image_path)\n","\n","    # Get the dimensions of the image\n","    width, height = img.size\n","\n","    # Resize by keeping the aspect ratio, but changing the dimension\n","    # so the shortest size is 255px\n","    # img = img.resize((255, int(255*(height/width))) if width < height else (int(255*(width/height)), 255))\n","\n","    # Get the dimensions of the new image size\n","    width, height = img.size\n","\n","    # Set the coordinates to do a center crop of 224 x 224\n","    #left = (width - 224)/2\n","    #top = (height - 224)/2\n","    #right = (width + 224)/2\n","    #bottom = (height + 224)/2\n","    #img = img.crop((left, top, right, bottom))\n","\n","    # Turn image into numpy array\n","    img = np.array(img)\n","\n","    # Make the color channel dimension first instead of last\n","    img = img.transpose((2, 0, 1))\n","\n","    # Make all values between 0 and 1\n","    img = img/255\n","\n","    # Normalize based on the preset mean and standard deviation\n","    img[0] = (img[0] - data_means[0])/data_stds[0]\n","    img[1] = (img[1] - data_means[1])/data_stds[1]\n","    img[2] = (img[2] - data_means[2])/data_stds[2]\n","\n","    # Add a fourth dimension to the beginning to indicate batch size\n","    img = img[np.newaxis,:]\n","\n","    # Turn into a torch tensor\n","    image = torch.from_numpy(img)\n","    image = image.float()\n","    return image\n","\n","# Using our model to predict the label\n","def predict(image, model):\n","    # Pass the image through our model\n","    output = model.forward(image)\n","\n","    # Reverse the log function in our output\n","    output = torch.exp(output)\n","\n","    # Get the top predicted class, and the output percentage for\n","    # that class\n","    probs, classes = output.topk(1, dim=1)\n","    return probs.item(), classes.item()\n","\n","# Show Image\n","def show_image(image):\n","    # Convert image to numpy\n","    image = image.numpy()\n","\n","    # Un-normalize the image with avg std and mean\n","    image[0] = image[0] * 0.2030 + 0.6601\n","\n","    # Print the image\n","    fig = plt.figure(figsize=(25, 4))\n","    plt.imshow(np.transpose(image[0], (1, 2, 0)))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train set size:  9957\n"],"name":"stdout"},{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/checkpoints/densenet161-8d451a50.pth\n","100%|██████████| 110M/110M [00:02<00:00, 50.8MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["1 / 319\n","2 / 319\n","3 / 319\n","4 / 319\n","5 / 319\n","6 / 319\n","7 / 319\n","8 / 319\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sv8vq8UPnTHv","colab_type":"code","colab":{}},"source":["from torch.utils.data import WeightedRandomSampler\n","list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1, 0.2, 0.99], 5, replacement=False))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U71Msd6msj_7","colab_type":"text"},"source":["# **RUN 1**\n","\n","**HYPERPARAMETERS**\n","\n","* Batch size: 25\n","* Epochs: 3\n","* Folds: 5\n","\n","\n","**RESULTS**\n","\n","* Final Epoch Average\n"," * Accuracy = 0.958531\n"," * Training Loss = 0.177712\n"," * Validation Loss = 0.112544\n","* Final Results\n"," * Accuracy = 0.587800\n"," * Training Loss = 0.134093\n"," * Validation Loss = 1.885516\n","\n","# **RUN 2**\n","\n","**HYPERPARAMETERS**\n","\n","* Batch size: 25\n","* Epochs: 10\n","* Folds: 5\n","\n","\n","**RESULTS**\n","\n","* Final Epoch Average\n"," * Accuracy = 0.996199\n"," * Training Loss = 0.048075\n"," * Validation Loss = 0.012868 \n","* Final Results\n"," * Accuracy = 0.564933\n"," * Training Loss = 0.053415\n"," * Validation Loss = 2.410395\n","\n"," "]},{"cell_type":"code","metadata":{"id":"JMytA1uPAf62","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}