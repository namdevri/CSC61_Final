{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22033,
     "status": "ok",
     "timestamp": 1575499850650,
     "user": {
      "displayName": "Joseph Erickson",
      "photoUrl": "",
      "userId": "18197697941711049711"
     },
     "user_tz": 300
    },
    "id": "ZXLMLqmhqGqt",
    "outputId": "3a30e37b-1063-4ffd-f6ec-8c1bb5f3de79"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#mount your drive.  Complete Oauth to authenticate\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hqJt5FExqfYg"
   },
   "outputs": [],
   "source": [
    "#unzip image folder\n",
    "#!unzip -uq \"/content/gdrive/My Drive/jpegs.zip\" -d \"/content/gdrive/My Drive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 762645,
     "status": "error",
     "timestamp": 1575500591286,
     "user": {
      "displayName": "Joseph Erickson",
      "photoUrl": "",
      "userId": "18197697941711049711"
     },
     "user_tz": 300
    },
    "id": "xC81Yd_KrO5M",
    "outputId": "b4b60ce5-eaca-4946-c9db-c4adb81f744f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Train our network\n",
    "def train_network(epochs, val_percent, train_batch_size, test_batch_size, eval_freq):\n",
    "\n",
    "    # Training set values (same for all data)\n",
    "    data_means = [0.6786, 0.6413, 0.6605]\n",
    "    data_stds = [0.2012, 0.2080, 0.1997]\n",
    "\n",
    "    transformations = transforms.Compose([\n",
    "    #    transforms.Resize(255),\n",
    "    #    transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),  # Transforms channels from 0- 255 -> 0-1.\n",
    "        transforms.Normalize(mean=data_means, std=data_stds)])\n",
    "\n",
    "    #epochs = 10\n",
    "    #val_percent = 0.2\n",
    "    #train_batch_size = 25\n",
    "    #test_batch_size = 25\n",
    "    #eval_freq = 1\n",
    "\n",
    "    #full_train_set = datasets.ImageFolder(\"/content/gdrive/My Drive/TRAIN\", transform=transformations)\n",
    "    full_train_set = datasets.ImageFolder(\"./jpegs/TRAIN\", transform=transformations)\n",
    "    #full_train_set, temp = torch.utils.data.random_split(full_train_set, [int(len(full_train_set) / 20), len(full_train_set) - int(len(full_train_set) / 20)])\n",
    "    full_train_loader = torch.utils.data.DataLoader(full_train_set, batch_size=train_batch_size, shuffle=True)\n",
    "    print(\"Full train set size: \", len(full_train_set))\n",
    "\n",
    "    val_size = int(len(full_train_set)*val_percent)\n",
    "    train_set, val_set = torch.utils.data.random_split(full_train_set, [len(full_train_set) - val_size, val_size])\n",
    "    print(\"Train set size: \", len(train_set))\n",
    "    print(\"Validation set size: \", len(val_set))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    #test_set = datasets.ImageFolder(\"/content/gdrive/My Drive/TEST\", transform=transformations)\n",
    "    test_set = datasets.ImageFolder(\"./jpegs/TEST\", transform=transformations)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=test_batch_size, shuffle=True)\n",
    "    print(\"Test set size: \", len(test_set))\n",
    "\n",
    "    # Options: MOST GENERALIZED - 121, 169, 201, 161 - MOST ACCURATE\n",
    "    # https://pytorch.org/hub/pytorch_vision_densenet/\n",
    "    model = models.densenet161(pretrained=True)\n",
    "\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    classifier_input = model.classifier.in_features\n",
    "    num_labels = 4\n",
    "    classifier = nn.Sequential(nn.Linear(classifier_input, 1024),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(1024, 512),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(512, num_labels),\n",
    "                               nn.LogSoftmax(dim=1))\n",
    "\n",
    "\n",
    "    model.classifier = classifier\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Can choose various loss functions to use:\n",
    "    #criterion = nn.NLLLoss()\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.MultiMarginLoss(margin=1.0)\n",
    "    \n",
    "    # Set the optimizer function using torch.optim as optim library\n",
    "    optimizer = optim.Adam(model.classifier.parameters())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        # Training the model\n",
    "        model.train()\n",
    "    #    counter = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Clear optimizers\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            output = model.forward(inputs)\n",
    "            # Loss\n",
    "            loss = criterion(output, labels)\n",
    "            # Calculate gradients (backpropogation)\n",
    "            loss.backward()\n",
    "            # Adjust parameters based on gradients\n",
    "            optimizer.step()\n",
    "            # Add the loss to the training set's rnning loss\n",
    "            train_loss += loss.item()*inputs.size(0)\n",
    "\n",
    "            # Print the progress of our training\n",
    "    #        counter += 1\n",
    "    #        if epoch == 0:\n",
    "    #            print(counter, \"/\", len(train_loader))\n",
    "\n",
    "        if (epoch % eval_freq == 0):\n",
    "\n",
    "            # Evaluating the model\n",
    "            model.eval()\n",
    "    #        counter = 0\n",
    "            # Tell torch not to calculate gradients\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    # Move to device\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    # Forward pass\n",
    "                    output = model.forward(inputs)\n",
    "                    # Calculate Loss\n",
    "                    valloss = criterion(output, labels)\n",
    "                    # Add loss to the validation set's running loss\n",
    "                    val_loss += valloss.item()*inputs.size(0)\n",
    "\n",
    "                    # Since our model outputs a LogSoftmax, find the real\n",
    "                    # percentages by reversing the log function\n",
    "                    output = torch.exp(output)\n",
    "                    # Get the top class of the output\n",
    "                    top_p, top_class = output.topk(1, dim=1)\n",
    "                    # See how many of the classes were correct?\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    # Calculate the mean (get the accuracy for this batch)\n",
    "                    # and add it to the running accuracy for this epoch\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "                    # Print the progress of our evaluation\n",
    "    #                counter += 1\n",
    "    #                if epoch == 0:\n",
    "    #                    print(counter, \"/\", len(val_loader))\n",
    "\n",
    "            # Get the average loss for the entire epoch\n",
    "            train_loss = train_loss/len(train_loader.dataset)\n",
    "            val_loss = val_loss/len(val_loader.dataset)\n",
    "            accuracy = accuracy/len(val_loader)\n",
    "            # Print out the information\n",
    "            print('Epoch: {} \\tAccuracy: {:.6f} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} '.format(epoch+1, accuracy, train_loss, val_loss))\n",
    "\n",
    "\n",
    "\n",
    "    full_train_loss = 0\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    # Training the model one final time on the full dataset\n",
    "    model.train()\n",
    "    #counter = 0\n",
    "    for inputs, labels in full_train_loader:\n",
    "        # Move to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Clear optimizers\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model.forward(inputs)\n",
    "        # Loss\n",
    "        loss = criterion(output, labels)\n",
    "        # Calculate gradients (backpropogation)\n",
    "        loss.backward()\n",
    "        # Adjust parameters based on gradients\n",
    "        optimizer.step()\n",
    "        # Add the loss to the training set's rnning loss\n",
    "        full_train_loss += loss.item()*inputs.size(0)\n",
    "\n",
    "        # Print the progress of our training\n",
    "     #   counter += 1\n",
    "     #   print(counter, \"/\", len(full_train_loader))\n",
    "\n",
    "    # Saving the model\n",
    "    torch.save(model, \"./blood_model_e{}_v{}_b{}.py\".format(epochs, val_percent, train_batch_size))\n",
    "\n",
    "    model.eval()\n",
    "    #counter = 0\n",
    "    # Tell torch not to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # Move to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            output = model.forward(inputs)\n",
    "            # Calculate Loss\n",
    "            testloss = criterion(output, labels)\n",
    "            # Add loss to the validation set's running loss\n",
    "            test_loss += testloss.item()*inputs.size(0)\n",
    "\n",
    "            # Since our model outputs a LogSoftmax, find the real\n",
    "            # percentages by reversing the log function\n",
    "            output = torch.exp(output)\n",
    "            # Get the top class of the output\n",
    "            top_p, top_class = output.topk(1, dim=1)\n",
    "            # See how many of the classes were correct?\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            # Calculate the mean (get the accuracy for this batch)\n",
    "            # and add it to the running accuracy for this epoch\n",
    "            test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "            # Print the progress of our evaluation\n",
    "    #        counter += 1\n",
    "    #        print(counter, \"/\", len(test_loader))\n",
    "\n",
    "    # Get the average loss for the entire fold\n",
    "    full_train_loss = full_train_loss/len(full_train_loader.dataset)\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    test_accuracy = test_accuracy/len(test_loader)\n",
    "    # Print out the information\n",
    "    #print('Test Set Accuracy: ', test_accuracy)\n",
    "    print('Final Results:\\tAccuracy: {:.6f} \\tTraining Loss: {:.6f} \\tTesting Loss: {:.6f} \\n'.format(test_accuracy, full_train_loss, test_loss))\n",
    "\n",
    "\n",
    "# Process our image\n",
    "def process_image(image_path):\n",
    "    # Load Image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Get the dimensions of the image\n",
    "    width, height = img.size\n",
    "\n",
    "    # Resize by keeping the aspect ratio, but changing the dimension\n",
    "    # so the shortest size is 255px\n",
    "    # img = img.resize((255, int(255*(height/width))) if width < height else (int(255*(width/height)), 255))\n",
    "\n",
    "    # Get the dimensions of the new image size\n",
    "    width, height = img.size\n",
    "\n",
    "    # Set the coordinates to do a center crop of 224 x 224\n",
    "    #left = (width - 224)/2\n",
    "    #top = (height - 224)/2\n",
    "    #right = (width + 224)/2\n",
    "    #bottom = (height + 224)/2\n",
    "    #img = img.crop((left, top, right, bottom))\n",
    "\n",
    "    # Turn image into numpy array\n",
    "    img = np.array(img)\n",
    "\n",
    "    # Make the color channel dimension first instead of last\n",
    "    img = img.transpose((2, 0, 1))\n",
    "\n",
    "    # Make all values between 0 and 1\n",
    "    img = img/255\n",
    "\n",
    "    # Normalize based on the preset mean and standard deviation\n",
    "    img[0] = (img[0] - data_means[0])/data_stds[0]\n",
    "    img[1] = (img[1] - data_means[1])/data_stds[1]\n",
    "    img[2] = (img[2] - data_means[2])/data_stds[2]\n",
    "\n",
    "    # Add a fourth dimension to the beginning to indicate batch size\n",
    "    img = img[np.newaxis,:]\n",
    "\n",
    "    # Turn into a torch tensor\n",
    "    image = torch.from_numpy(img)\n",
    "    image = image.float()\n",
    "    return image\n",
    "\n",
    "# Using our model to predict the label\n",
    "def predict(image, model):\n",
    "    # Pass the image through our model\n",
    "    output = model.forward(image)\n",
    "\n",
    "    # Reverse the log function in our output\n",
    "    output = torch.exp(output)\n",
    "\n",
    "    # Get the top predicted class, and the output percentage for\n",
    "    # that class\n",
    "    probs, classes = output.topk(1, dim=1)\n",
    "    return probs.item(), classes.item()\n",
    "\n",
    "# Show Image\n",
    "def show_image(image):\n",
    "    # Convert image to numpy\n",
    "    image = image.numpy()\n",
    "\n",
    "    # Un-normalize the image with avg std and mean\n",
    "    image[0] = image[0] * 0.2030 + 0.6601\n",
    "\n",
    "    # Print the image\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    plt.imshow(np.transpose(image[0], (1, 2, 0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Epochs 5 \tBatch Size: 25\n",
      "Full train set size:  9957\n",
      "Train set size:  7966\n",
      "Validation set size:  1991\n",
      "Test set size:  2487\n"
     ]
    }
   ],
   "source": [
    "t_epochs = [5,10,25,50,100]\n",
    "t_batches = [25,50,100,200]\n",
    "\n",
    "for i in t_epochs:\n",
    "    for j in t_batches:\n",
    "        print('\\nNumber of Epochs {} \\tBatch Size: {}'.format(i,j))\n",
    "        train_network(epochs=i, val_percent=0.2, train_batch_size=j, test_batch_size=j, eval_freq=1)\n",
    "\n",
    "#train_network(epochs=3, val_percent=0.2, train_batch_size=25, test_batch_size=25, eval_freq=1)\n",
    "\n",
    "#epochs = 10\n",
    "#val_percent = 0.2\n",
    "#train_batch_size = 25\n",
    "#test_batch_size = 25\n",
    "#eval_freq = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sv8vq8UPnTHv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9957 9957\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _thnn_conv2d_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-cdebca498805>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mfull_train_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_preds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_train_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mpreds_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_num_correct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_train_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_train_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-cdebca498805>\u001b[0m in \u001b[0;36mget_all_preds\u001b[1;34m(model, loader)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         all_preds = torch.cat(\n\u001b[0;32m     11\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\models\\densenet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 342\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _thnn_conv2d_forward"
     ]
    }
   ],
   "source": [
    "'''print(len(full_train_set),len(full_train_set.targets))\n",
    "\n",
    "# https://deeplizard.com/learn/video/0LhiS6yu2qQ\n",
    "@torch.no_grad()\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat(\n",
    "            (all_preds, preds)\n",
    "            ,dim=0\n",
    "        )\n",
    "    return all_preds\n",
    "\n",
    "with torch.no_grad():\n",
    "    full_train_preds = get_all_preds(model, full_train_loader)\n",
    "    \n",
    "preds_correct = get_num_correct(full_train_preds, full_train_set.targets)\n",
    "\n",
    "print('total correct:', preds_correct)\n",
    "print('accuracy:', preds_correct / len(full_train_set))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab_type": "text",
    "id": "U71Msd6msj_7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Run 1\n",
    "\n",
    "epochs = 10\n",
    "val_percent = 0.2\n",
    "train_batch_size = 25\n",
    "test_batch_size = 25\n",
    "eval_freq = 1\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "\n",
    "Epoch: 1 \tAccuracy: 0.792156 \tTraining Loss: 0.866748 \tValidation Loss: 0.507381 \n",
    "\n",
    "Epoch: 2 \tAccuracy: 0.828156 \tTraining Loss: 0.480419 \tValidation Loss: 0.436654 \n",
    "\n",
    "Epoch: 3 \tAccuracy: 0.832219 \tTraining Loss: 0.411089 \tValidation Loss: 0.421812 \n",
    "\n",
    "Epoch: 4 \tAccuracy: 0.882937 \tTraining Loss: 0.364378 \tValidation Loss: 0.311798 \n",
    "\n",
    "Epoch: 5 \tAccuracy: 0.884000 \tTraining Loss: 0.334123 \tValidation Loss: 0.299411 \n",
    "\n",
    "Epoch: 6 \tAccuracy: 0.839937 \tTraining Loss: 0.283902 \tValidation Loss: 0.376899 \n",
    "\n",
    "Epoch: 7 \tAccuracy: 0.897500 \tTraining Loss: 0.245882 \tValidation Loss: 0.265919 \n",
    "\n",
    "Epoch: 8 \tAccuracy: 0.845719 \tTraining Loss: 0.243905 \tValidation Loss: 0.433223 \n",
    "\n",
    "Epoch: 9 \tAccuracy: 0.909000 \tTraining Loss: 0.221514 \tValidation Loss: 0.226245 \n",
    "\n",
    "Epoch: 10 \tAccuracy: 0.908156 \tTraining Loss: 0.224361 \tValidation Loss: 0.237138 \n",
    "\n",
    "\n",
    "Test Set Accuracy:  0.609033337533474\n",
    "Training Loss: 0.214690 \tTesting Loss: 1.398789\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMytA1uPAf62"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DSP 461 Final Project(v5).ipynb",
   "provenance": [
    {
     "file_id": "1fF72BlAm7vwFnC9cCnAjMHb_0ZhK23wy",
     "timestamp": 1575498877653
    },
    {
     "file_id": "1yyL8Uo9SzZhV6HtAEuEGPYNoUlOYa3aK",
     "timestamp": 1574796892120
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
